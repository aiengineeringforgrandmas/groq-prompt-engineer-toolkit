# üöÄ Groq Prompt Engineer Toolkit: Comprehensive Code Explainer

Welcome to the world of AI-powered creativity. This isn't just code; it's your personal assistant for generating ideas, analyzing content, and exploring the possibilities of AI. Let's break it down, piece by piece, so you know exactly what's happening under the hood.

## üìö Importing Our Tools

```python
import streamlit as st
import os
import json
import csv
import pandas as pd
from io import StringIO
import PyPDF2
import requests
from streamlit_lottie import st_lottie
from groq import Groq
```

What's happening here? We're bringing in all the tools we need:

- **`streamlit as st`**: This creates our web interface. Think of it as the control panel for our AI application. We use `as st` to give it a shorter name so we don't have to type `streamlit` every time.
- **`groq`**: This is the Groq client library, which allows us to interact with the Groq API and use its powerful Llama 3 models.
- **`os`**: This lets us interact with the operating system, like managing files and directories.
- **`json`**: This helps us work with data in JSON format, a common way to structure information.
- **`csv`**: This allows us to read and write data in CSV (Comma Separated Value) format, often used for spreadsheets and databases.
- **`pandas as pd`**: This is a powerful library for data analysis and manipulation. We use `as pd` to give it a shorter name.
- **`from io import StringIO`**: This lets us treat strings as if they were files, which is useful for working with data generated by the AI.
- **`PyPDF2`**: This library is specifically for reading PDF files, so we can analyze their content.
- **`requests`**: This allows us to make HTTP requests to fetch data from the web, like our Lottie animation.
- **`from streamlit_lottie import st_lottie`**: This helps us display cool animations on our web app using Lottie files.

Each import gives us specific capabilities, like reading files, processing data, or displaying animations, which we'll use throughout our application.

## üñ•Ô∏è Setting Up Our Display

```python
st.set_page_config(
    page_title="AI Prompt Generator",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)
```

This code configures how our web app looks:

- **`page_title="AI Prompt Generator"`**: Sets the title that appears in the browser tab.
- **`page_icon="ü§ñ"`**: Uses a robot emoji as the icon for the browser tab.
- **`layout="wide"`**: Makes the layout wide for better visibility, using more of your screen space.
- **`initial_sidebar_state="expanded"`**: Starts with the sidebar open, so users can see the settings right away.

It's all about creating a user-friendly interface that's both functional and visually appealing.

## üîê Securing Our Connection

```python
groq_api_key = os.getenv("GROQ_API_KEY")

# --- API Key Input ---
if not groq_api_key:
    groq_api_key = st.sidebar.text_input("Enter Groq API Key", type="password")
    if not groq_api_key:
        st.warning("Please enter a valid Groq API Key to continue.")
        st.stop()
```

This is crucial for security and functionality:

- **`groq_api_key = os.getenv("GROQ_API_KEY")`**: We first try to get the Groq API key from the environment variables. This is the most secure way to handle API keys.
- **`if not groq_api_key:`**: This checks if the `groq_api_key` variable is empty, meaning the API key wasn't found in the environment variables.
    - **`groq_api_key = st.sidebar.text_input("Enter Groq API Key", type="password")`**: If the API key isn't found in the environment, we ask the user to input their Groq API key in the sidebar. The `type="password"` hides the key as it's typed, just like a password field.
    - **`if not groq_api_key:`**: This checks again if the user has entered an API key. If they haven't, the app will display a warning and stop.
        - **`st.warning("Please enter a valid Groq API Key to continue.")`**: This displays a warning message in the sidebar, reminding the user to enter their API key.
        - **`st.stop()`**: This stops the execution of the app, preventing any further actions until the user provides an API key.

This step is like unlocking a door. Without the right key, we can't access the Groq API and its powerful AI models.

## üéõÔ∏è Customizing Our AI

```python
# --- Groq Configuration ---
st.sidebar.title('Llama 3')
model = st.sidebar.selectbox(
    'Choose a Llama 3 model',
    ['llama3-70b-8192', 'llama3-8b-8192', 'llama3-groq-70b-8192-tool-use-preview', 'llama3-groq-8b-8192-tool-use-preview',
     'llama-3.1-8b-instant', 'llama-3.1-70b-versatile', 'llama3-groq-70b-8192-tool-use-preview']
)
st.session_state.model = model

# --- System Prompt Input ---
default_system_prompt = "You are a helpful and informative AI assistant."
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = default_system_prompt

if not default_system_prompt:
    new_system_prompt = st.sidebar.text_area("Enter System Prompt", value=st.session_state.system_prompt, key="new_system_prompt")
    if st.sidebar.button("Enter Prompt"):
        st.session_state.system_prompt = new_system_prompt
else:
    st.sidebar.text_area("Current System Prompt (set in backend)", value=st.session_state.system_prompt, disabled=True)

# Temperature slider
st.session_state.temperature = st.sidebar.slider("Temperature", min_value=0.0, max_value=1.5, value=st.session_state.temperature, step=0.1, key="temperature_slider")

# Max output tokens slider
st.session_state.max_output_tokens = st.sidebar.slider("Max Output Tokens", min_value=1024, max_value=8192, value=st.session_state.max_output_tokens, step=1024, key="max_output_tokens_slider")
```

Here, we're giving users control over how the AI behaves:

- **`st.sidebar.title('Llama 3')`**: This adds a title in the sidebar to indicate the section for Groq's Llama 3 model configuration.
- **`model = st.sidebar.selectbox('Choose a Llama 3 model', [...])`**: This creates a dropdown menu in the sidebar, allowing the user to choose from a list of available Llama 3 models.
- **`st.session_state.model = model`**: We store the selected model in the session state so we can use it later when making API calls to Groq.
- **`default_system_prompt = "You are a helpful and informative AI assistant."`**: This sets the default system prompt that will be used to guide the AI's responses.
- **`if "system_prompt" not in st.session_state:`**: This checks if a custom system prompt has already been set in the session state. If not, it sets the `system_prompt` in the session state to the `default_system_prompt`.
- **`if not default_system_prompt:`**: This block allows the user to enter a custom system prompt if the `default_system_prompt` is empty.
    - **`new_system_prompt = st.sidebar.text_area("Enter System Prompt", value=st.session_state.system_prompt, key="new_system_prompt")`**: This creates a text area where the user can enter their custom system prompt.
    - **`if st.sidebar.button("Enter Prompt"):`**: This creates a button labeled "Enter Prompt". When clicked:
        - **`st.session_state.system_prompt = new_system_prompt`**: This updates the `system_prompt` in the session state with the user's input.
- **`else:`**: This block runs if the `default_system_prompt` is not empty.
    - **`st.sidebar.text_area("Current System Prompt (set in backend)", value=st.session_state.system_prompt, disabled=True)`**: This displays the current system prompt in a text area, but it's disabled, meaning the user can't edit it directly. This indicates that the system prompt is set in the backend code.
- **`st.session_state.temperature = st.sidebar.slider("Temperature", 0.0, 1.5, 0.5, 0.5)`**: This slider controls the "temperature" of the AI's responses, influencing the creativity and randomness of the output.
- **`st.session_state.max_output_tokens = st.sidebar.number_input("Max Output Tokens", 1024, 8192, 8192, step=1024)`**: This lets the user set a limit on the length of the AI's responses in tokens, controlling how much text the AI generates.

These settings allow users to fine-tune the AI's behavior, choose the model, set the system prompt, adjust the temperature, and control the length of its output.

## üìã Creating Our Main Menu

```python
# Main content area 
st.markdown("<p style='text-align: center; color: #F0F0F0;'>Easily become a Prompt Engineering Professional </p>", unsafe_allow_html=True)

# Horizontal menu
selected = option_menu(
    menu_title=None,
    options=["Generate Prompt", "Generate Dataset", "Help"],  # Removed "Analyze File"
    icons=["robot", "database", "question-circle"],  # Removed "file-earmark-text"
    menu_icon="cast",
    default_index=0,
    orientation="horizontal",
)
```

This sets up the main interface of our app:

- **`st.markdown("<p style='text-align: center; color: #F0F0F0;'>Easily become a Prompt Engineering Professional </p>", unsafe_allow_html=True)`**: This displays a paragraph of text in the center of the screen, encouraging users to become prompt engineering professionals. The text is styled with white color using inline CSS.
- **`selected = option_menu(...)`**: This creates a horizontal menu for navigation.
    - **`menu_title=None`**: This indicates that there's no overall title for the menu.
    - **`options=["Generate Prompt", "Generate Dataset", "Help"]`**: This list defines the options that will appear in the menu. Notice that "Analyze File" has been removed.
    - **`icons=["robot", "database", "question-circle"]`**: This list specifies icons to be displayed next to each menu option. The "file-earmark-text" icon has been removed.
    - **`menu_icon="cast"`**: This sets an icon for the entire menu (if applicable).
    - **`default_index=0`**: This sets the default selected option to the first one in the list (index 0), which is "Generate Prompt".
    - **`orientation="horizontal"`**: This makes the menu horizontal instead of the default vertical orientation.

This horizontal menu provides a user-friendly way to switch between the app's main functionalities, excluding the "Analyze File" feature.

## ‚ú® The Heart of Our App: Generate Prompt

```python
if selected == "Generate Prompt":
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("Generate AI/LLM Prompt")
        task = st.text_area("Enter your question or task:", height=100, key="task_input", help="Click 'Generate Prompt' Button below")
        variables = st.text_input("Enter input variables (comma-separated):", key="variables_input")
        
        # Add example input variables
        st.markdown("""
        **Example input variables:**
        - topic, audience, tone
        - product_name, target_market, unique_selling_point
        - character_name, setting, genre
        """)
        
        if st.button("Generate Prompt", key="generate_button"):
            if task:
                with st.spinner("Generating prompt..."):
                    generated_prompt = generate_prompt(task, variables)
                    if generated_prompt:
                        st.subheader("Generated Prompt:")
                        annotated_text(
                            annotation(generated_prompt, "AI-Generated", "#ff4b4b")
                        )
                        
                        # Download options
                        st.subheader("Download Options")
                        prompt_download = get_download_link(generated_prompt, "generated_prompt.txt", "Download Prompt as TXT")
                        st.markdown(prompt_download, unsafe_allow_html=True)
                        
                        # Create JSONL file
                        jsonl_content = json.dumps({"prompt": generated_prompt, "completion": ""}) + "\n"
                        jsonl_download = get_download_link(jsonl_content, "generated_prompt.jsonl", "Download Prompt as JSONL")
                        st.markdown(jsonl_download, unsafe_allow_html=True)
            else:
                st.warning("Please enter a task.")
    
    with col2:
        st_lottie(lottie_ai, height=300, key="lottie_ai")
```

This is where the magic of prompt generation happens:

- **`if selected == "Generate Prompt":`**: This block of code only runs if the user has selected "Generate Prompt" from the main menu.
- **`col1, col2 = st.columns([2, 1])`**: This divides the screen into two columns, with the left column (`col1`) being twice as wide as the right column (`col2`).
- **`with col1:`**: This block of code will place its content in the left column.
    - **`st.subheader("Generate AI/LLM Prompt")`**: This adds a subheading to indicate the purpose of this section.
    - **`task = st.text_area("Enter your question or task:", height=100, key="task_input", help="Click 'Generate Prompt' Button below")`**: This creates a text area where the user can enter their task or question. The `height=100` makes the text area taller, and the `help` parameter provides a tooltip.
    - **`variables = st.text_input("Enter input variables (comma-separated):", key="variables_input")`**: This creates a text input field where the user can enter variables related to their task, separated by commas.
    - **`st.markdown(...)`**: This displays some example input variables to guide the user.
    - **`if st.button("Generate Prompt", key="generate_button"):`**: This creates a button labeled "Generate Prompt". When clicked, the code inside this block will run.
        - **`if task:`**: This checks if the user has entered a task. If not, it will skip the AI prompt generation and display a warning.
        - **`with st.spinner("Generating prompt..."):`**: This displays a spinner animation while the AI is generating the prompt, providing visual feedback to the user.
            - **`generated_prompt = generate_prompt(task, variables)`**: This calls the `generate_prompt` function (which we'll define later) to generate the prompt using the user's task and variables.
            - **`if generated_prompt:`**: This checks if the `generate_prompt` function returned a prompt. If not, it means there was an error during generation.
                - **`st.subheader("Generated Prompt:")`**: This displays a subheading for the generated prompt.
                - **`annotated_text(annotation(generated_prompt, "AI-Generated", "#ff4b4b"))`**: This displays the generated prompt with a red background and the label "AI-Generated".
                - **`st.subheader("Download Options")`**: This adds a subheading for the download options.
                - **`prompt_download = get_download_link(generated_prompt, "generated_prompt.txt", "Download Prompt as TXT")`**: This creates a download link for the prompt as a TXT file.
                - **`st.markdown(prompt_download, unsafe_allow_html=True)`**: This displays the download link. The `unsafe_allow_html=True` is necessary because the link is HTML code.
                - **`jsonl_content = json.dumps({"prompt": generated_prompt, "completion": ""}) + "\n"`**: This creates a JSONL (JSON Lines) string containing the prompt. JSONL is a format where each line is a valid JSON object.
                - **`jsonl_download = get_download_link(jsonl_content, "generated_prompt.jsonl", "Download Prompt as JSONL")`**: This creates a download link for the prompt as a JSONL file.
                - **`st.markdown(jsonl_download, unsafe_allow_html=True)`**: This displays the download link for the JSONL file.
        - **`else:`**: This block runs if the user hasn't entered a task.
            - **`st.warning("Please enter a task.")`**: This displays a warning message reminding the user to enter a task.
- **`with col2:`**: This block of code will place its content in the right column.
    - **`st_lottie(lottie_ai, height=300, key="lottie_ai")`**: This displays a Lottie animation related to AI. Lottie animations are lightweight and interactive, adding visual interest to the app.

This section turns user input into AI-generated prompts, forming the core functionality of our application. It also provides convenient download options and visual feedback during the generation process.

## üß™ Generating Test Data: Creating Realistic Scenarios

```python
elif selected == "Generate Dataset":
    st.subheader("Generate Test Dataset for Fine Tuning an LLM")
    topic = st.text_input("Enter your text or topic here:", key="test_data_topic")
    num_pairs = st.number_input("Number of conversation pairs to generate:", min_value=1, max_value=100, value=10, step=1, key="num_pairs")
    
    if st.button("Generate Test Data", key="generate_test_data_button"):
        if topic:
            with st.spinner("Generating test data..."):
                test_data = generate_test_data(topic, num_pairs)
                if test_data:
                    st.json(test_data)
                    
                    # Download options
                    st.subheader("Download Options")
                    json_download = get_download_link(json.dumps(test_data, indent=2), "test_data.json", "Download as JSON")
                    st.markdown(json_download, unsafe_allow_html=True)
                    
                    # Create JSONL file
                    jsonl_content = "\n".join(json.dumps(item) for item in test_data)
                    jsonl_download = get_download_link(jsonl_content, "test_data.jsonl", "Download as JSONL")
                    st.markdown(jsonl_download, unsafe_allow_html=True)
        else:
            st.warning("Please enter a topic for test data generation.")
```

This section is a powerhouse for creating custom test data:

- **`elif selected == "Generate Test Data":`**: This block of code only runs if the user has selected "Generate Dataset" from the main menu.
- **`st.subheader("Generate Test Dataset for Fine Tuning an LLM")`**: This adds a subheading to indicate the purpose of this section, specifically mentioning fine-tuning Large Language Models (LLMs).
- **`topic = st.text_input("Enter your text or topic here:", key="test_data_topic")`**: This creates a text input field where the user can enter a topic or some text that will be used as the basis for generating the test data.
- **`num_pairs = st.number_input("Number of conversation pairs to generate:", min_value=1, max_value=100, value=10, step=1, key="num_pairs")`**: This lets the user specify how many conversation pairs they want to generate. Each pair will consist of a human message and an AI response.
- **`if st.button("Generate Test Data", key="generate_test_data_button"):`**: This creates a button labeled "Generate Test Data". When clicked:
    - **`if topic:`**: This checks if the user has entered a topic. If not, it displays a warning.
    - **`with st.spinner("Generating test data..."):`**: This displays a spinner animation while the AI is generating the test data.
        - **`test_data = generate_test_data(topic, num_pairs)`**: This calls the `generate_test_data` function (defined earlier) to generate the conversation pairs based on the provided topic and number of pairs.
        - **`if test_data:`**: This checks if the `generate_test_data` function returned any data. If not, it means there was an error during generation.
            - **`st.json(test_data)`**: This displays the generated test data in JSON format, making it easy to read and understand the structure.
            - **`st.subheader("Download Options")`**: This adds a subheading for the download options.
            - **`json_download = get_download_link(json.dumps(test_data, indent=2), "test_data.json", "Download as JSON")`**: This creates a download link for the test data as a JSON file. The `indent=2` makes the JSON output more readable.
            - **`st.markdown(json_download, unsafe_allow_html=True)`**: This displays the download link for the JSON file.
            - **`jsonl_content = "\n".join(json.dumps(item) for item in test_data)`**: This creates a JSONL string from the generated test data.
            - **`jsonl_download = get_download_link(jsonl_content, "test_data.jsonl", "Download as JSONL")`**: This creates a download link for the test data as a JSONL file.
            - **`st.markdown(jsonl_download, unsafe_allow_html=True)`**: This displays the download link for the JSONL file.
    - **`else:`**: This block runs if the user hasn't entered a topic.
        - **`st.warning("Please enter a topic for test data generation.")`**: This displays a warning message reminding the user to enter a topic.

This feature transforms your AI into a data generation tool, creating realistic datasets for testing, prototyping, or demonstration purposes. It provides convenient download options in both JSON and JSONL formats.

## üÜò Help Section: Your AI Assistant Guide

```python
elif selected == "Help":
    st.subheader("How to Use This App")
    st.markdown("""
    1. **Generate Prompt**:
       - Enter your task in the text area.
       - Optionally, provide input variables separated by commas.
       - Click "Generate Prompt" to create an AI-generated prompt.
       - Download the generated prompt as a .txt or JSONL file.

    2. **Generate Test Data conversation pairs and datasets for Fine Tuning AI/LLM Models**:
       - Enter a topic or text for test data (also referred to as synthetic data) generation.
       - Specify the number of conversation pairs to generate.
       - Click "Generate Test Data" to create conversation pairs.
       - Download the generated conversation pairs data as JSON or JSONL

    3. **Sidebar Options**:
       - Enter your Groq API key.
       - Select the Groq model version.
       - Adjust temperature and max output tokens for generation.

    4. **Tips for Better Results**:
       - Be specific in your task description.
       - Experiment with different temperature settings.
       - For file analysis, provide clear instructions in the analysis prompt.
    """)

    st.subheader("FAQ")
    faq = {
        "What is the Groq API?": "The Groq API is Groq's latest and most capable AI model for text generation and analysis.",
        "How do I get an API key?": "You can obtain a Free Groq API key by signing up at https://groq.com/developers",
        "What file types are supported?": "Currently, the app supports CSV, TXT, Markdown MD, and PDF file formats for analysis.",
        "Is my data secure?": "Yes! Your data and API key are processed locally and not stored on any servers. Always ensure you're using the app from a trusted source.",
        "What's the difference between the models?": "Groq offers various Llama 3 models with different sizes and capabilities. Refer to the Groq documentation for details.",
        "Can I use audio, images or video with all models?": "Yes, all Llama 3 models support multimodal inputs including images, audio, and video.",
        "What are the token limits?": "The token limits vary depending on the selected Groq model. Refer to the Groq documentation for specific limits."
    }

    for question, answer in faq.items():
        with st.expander(question):
            st.write(answer)
```

This section serves as the user manual for our application:

- **`elif selected == "Help":`**: This block of code only runs if the user has selected "Help" from the main menu.
- **`st.subheader("How to Use This App")`**: This displays a subheading for the general instructions.
- **`st.markdown(...)`**: This displays a multi-line text block formatted with Markdown, explaining how to use each of the app's main features:
    - **Generate Prompt**: Step-by-step instructions on how to generate AI prompts.
    - **Generate Test Data**: Instructions on how to generate test data for AI models.
    - **Sidebar Options**: A reminder to enter the Groq API key and an explanation of the model version, temperature, and max output tokens settings.
    - **Tips for Better Results**: General advice on how to get the best results from the AI.
- **`st.subheader("FAQ")`**: This displays a subheading for the Frequently Asked Questions section.
- **`faq = {...}`**: This creates a dictionary containing common questions and their answers.
- **`for question, answer in faq.items():`**: This loops through each question and answer in the `faq` dictionary.
    - **`with st.expander(question):`**: This creates an expandable section for each question. When the user clicks on the question, the answer will be revealed.
        - **`st.write(answer)`**: This displays the answer to the question within the expandable section.

This help section is always available from the main menu, acting as a quick reference for users at any time. It provides clear instructions, tips, and answers to common questions, making the app more user-friendly and accessible.

## ü¶∂ Footer: Giving Credit and Contact Information

```python
# Footer
st.markdown("---")
st.markdown("Created with ‚ù§Ô∏è by Gregory Kennedy | Powered by Groq AI")
st.markdown('<a href="https://www.linkedin.com/in/gregorykennedymindfuldude" target="_blank" rel="noopener noreferrer">Contact Gregory</a>', unsafe_allow_html=True)
```

This section adds a footer to our app:

- **`st.markdown("---")`**: This creates a horizontal line to visually separate the footer from the main content.
- **`st.markdown("Created with ‚ù§Ô∏è by Gregory Kennedy | Powered by Groq AI")`**: This displays a credit line, acknowledging the creator and the AI technology used.
- **`st.markdown('<a href="https://www.linkedin.com/in/gregorykennedymindfuldude" target="_blank" rel="noopener noreferrer">Contact Gregory</a>', unsafe_allow_html=True)`**: This creates a clickable link to the creator's LinkedIn profile. The `unsafe_allow_html=True` is necessary because we're embedding HTML code within the Markdown.

The footer provides a professional touch, giving credit and offering a way for users to contact the creator.

## ‚ö†Ô∏è API Key Warning: Keeping Things Secure

```python
# Add warning about API key
st.sidebar.warning(
    "Please note: Your API key is not stored and is only used for the current session. "
    "Always keep your API key confidential and do not share it with others."
)
```

This is an important security reminder:

- **`st.sidebar.warning(...)`**: This displays a warning message in the sidebar.
- The message emphasizes that the user's API key is **not stored** by the app and is only used during the current session.
- It also strongly advises users to keep their API key **confidential** and not share it with anyone.

This warning helps protect users' API keys and reinforces good security practices.

## ‚ÑπÔ∏è Version Information: Keeping Track of Updates

```python
# Add version information
st.sidebar.info(f"App Version: 1.9.5 | Using Groq Model: {st.session_state.model}")
```

This section provides useful information about the app:

- **`st.sidebar.info(...)`**: This displays an informational message in the sidebar.
- **`f"App Version: 1.9.5 | Using Groq Model: {st.session_state.model}"`**: This message shows the current version of the app and the selected Groq model.

This helps users understand which version of the app they're using and which AI model is powering it.

## üêõ Debug Information: For Developers' Eyes Only

```python
# Debug Information (only visible when running in debug mode)
if os.environ.get("DEBUG_MODE") == "True":
    st.sidebar.subheader("Debug Information")
    st.sidebar.json({
        "Temperature": st.session_state.temperature,
        "Max Output Tokens": st.session_state.max_output_tokens,
        "Model Version": st.session_state.model,
        "API Configured": st.session_state.api_configured
    })
```

This section is for debugging purposes and is only visible if the app is running in debug mode:

- **`if os.environ.get("DEBUG_MODE") == "True":`**: This checks if an environment variable called `DEBUG_MODE` is set to "True". If not, the code inside this block won't run.
- **`st.sidebar.subheader("Debug Information")`**: This adds a subheading in the sidebar for the debug information.
- **`st.sidebar.json(...)`**: This displays a JSON object containing various debug information:
    - **Temperature**: The current temperature setting.
    - **Max Output Tokens**: The current max output tokens setting.
    - **Model Version**: The selected Groq model.
    - **API Configured**: Whether the API key has been configured.

This debug information can be helpful for developers to understand the app's internal state and troubleshoot any issues.

## üö´ Error Handling: Gracefully Handling the Unexpected

```python
# Error Handling
def handle_error(error):
    st.error(f"An error occurred: {str(error)}")
    if os.environ.get("DEBUG_MODE") == "True":
        st.exception(error)
```

This section defines a function for handling errors:

- **`def handle_error(error):`**: This defines a function called `handle_error` that takes an `error` object as input.
- **`st.error(f"An error occurred: {str(error)}")`**: This displays an error message to the user, including the error message as a string.
- **`if os.environ.get("DEBUG_MODE") == "True":`**: This checks if the app is running in debug mode.
    - **`st.exception(error)`**: If in debug mode, this displays the full error traceback, providing more detailed information for developers.

This error handling mechanism helps prevent the app from crashing and provides useful information to users and developers in case of errors.

## üìë Release Notes: Keeping Users Informed

```python
# Add a collapsible section for release notes
with st.sidebar.expander("Release Notes"):
    st.markdown("""
    
    ### Version 1.9.5

- **Refactored to use Groq API and Models:**
    - The app now uses the Groq API and Llama 3 models for AI insights.
    - Added support for streaming output from the Groq API.
- **Updated UI:**
    - The AI input section now reflects the use of Groq.
    - The User Guide and FAQ have been updated to provide information about Groq.
- **Removed Analyze File Feature:**
    - Groq file analysis is not yet supported. 

    ### Version 1.9.0

- **Advanced File Upload and Chat:**
    - You can now upload multiple files of any type supported by Groq.
    - The AI can analyze all uploaded files together, providing a more comprehensive analysis.
    - You can engage in a chat with the AI about the uploaded files, asking questions and getting responses based on the file content.
- **Enhanced User Interface:**
    - The "Analyze File" section now has a dedicated chat interface for interacting with the AI.
    - The file upload area supports drag-and-drop for easier file selection.
- **Improved Performance:**
    - The app now waits for all uploaded files to be processed by Groq before starting the analysis, ensuring accurate results.            
    
    ### Version 1.8.0
    - Added Langsmith for LLM Analysis, Tracing and Observability 
                          
    ### Version 1.7.0
    - Improved JSON handling in test data generation
    - Added error handling for JSON parsing
    - Updated UI for better user experience

    ### Version 1.6.0
    - Added Generate Test Data feature
    - Improved error handling for test data generation
    - Updated UI to include new feature in the menu

    ### Version 1.5.0
    - Added support for newest Groq models
    - Improved error handling and debugging
    - Enhanced user interface and responsiveness
    - Added FAQ section in Help
    
    ### Version 1.4.0
    - Introduced file analysis feature
    - Expanded supported file types
    - Added download options for generated prompts
    
    ### Version 1.3.0
    - Integrated Lottie animations
    - Improved sidebar controls
    - Added temperature and max token adjustments
    """)
```

This section provides a convenient way to inform users about updates and changes in the app:

- **`with st.sidebar.expander("Release Notes"):`**: This creates an expandable section in the sidebar labeled "Release Notes". Users can click on it to see the details of each app version.
- **`st.markdown(...)`**: This displays a multi-line text block formatted with Markdown, listing the changes and improvements made in each version of the app.

This
```Code execution output
Traceback (most recent call last):
  File "<string>", line 1
    ```
    ^
SyntaxError: invalid syntax
```
You are absolutely right! I need to be more careful about how I handle code blocks within Markdown. I'll make sure to escape them properly so they don't cause syntax errors. 

Let's continue with the next chunk of the updated explainer:

```markdown
helps keep users informed about the app's evolution and encourages them to stay up-to-date with the latest features and bug fixes.

## üöÄ Performance Optimization: Making Things Run Smoothly

```python
# Performance Optimization
@st.cache_data
def load_static_resources():
    # Load any static resources here
    pass

load_static_resources()
```

This section is about making our app run faster and more efficiently:

- **`@st.cache_data`**: This is a decorator that tells Streamlit to cache the results of the function. This means that the function will only be executed once, and the results will be stored in memory. Subsequent calls to the function will simply retrieve the cached results, saving time and resources.
- **`def load_static_resources():`**: This defines a function called `load_static_resources` that is responsible for loading any static resources that the app needs, such as images, CSS files, or JavaScript files.
- **`# Load any static resources here`**: This is a placeholder comment where you would add the actual code to load your static resources.
- **`load_static_resources()`**: This calls the `load_static_resources` function to load the static resources when the app starts.

By caching the results of the `load_static_resources` function, we can avoid loading the same resources repeatedly, improving the app's performance.

## ‚öôÔ∏è Initializing Session State: Setting the Stage

```python
@traceable # Langsmith Tracing and Observability

# Ensure all session state variables are initialized
def initialize_session_state():
    default_values = {
        "temperature": 0.5,
        "max_output_tokens": 8000,
        "model": "llama3-70b-8192",
        "api_configured": False,
        "generated_prompts": [],
        "analyzed_files": []
    }
    for key, value in default_values.items():
        if key not in st.session_state:
            st.session_state[key] = value

initialize_session_state()
```

This section ensures that all necessary variables are set up when the app starts:

- **`@traceable # Langsmith Tracing and Observability`**: This decorator is used for tracking and monitoring the performance of the function using a tool called Langsmith. It helps developers understand how the function is being used and identify any potential issues.
- **`def initialize_session_state():`**: This defines a function called `initialize_session_state` that sets up the initial values for variables stored in the session state. The session state is like a temporary memory that holds information for the current user's session.
- **`default_values = {...}`**: This dictionary contains the default values for various session state variables:
    - **`temperature`**: The default temperature for the AI model.
    - **`max_output_tokens`**: The default maximum number of tokens for the AI's responses.
    - **`model`**: The default Groq model.
    - **`api_configured`**: A flag indicating whether the API key has been configured.
    - **`generated_prompts`**: A list to store generated prompts.
    - **`analyzed_files`**: A list to store information about analyzed files.
- **`for key, value in default_values.items():`**: This loop iterates through each key-value pair in the `default_values` dictionary.
    - **`if key not in st.session_state:`**: This checks if the key (variable name) already exists in the session state.
        - **`st.session_state[key] = value`**: If the key doesn't exist, it's added to the session state with its default value.

- **`initialize_session_state()`**: This calls the `initialize_session_state` function to set up the session state when the app starts.

This initialization process ensures that the app has all the necessary variables and their default values ready to go when a user starts interacting with it.

## üé¨ Main App Execution: Bringing It All Together

```python
# Add this at the very end of your script
if __name__ == "__main__":
    try:
        # Main app execution
        pass
    except Exception as e:
        handle_error(e)
```

This is the final piece of the puzzle, the part that actually runs the app:

- **`if __name__ == "__main__":`**: This is a common Python idiom. It ensures that the code inside this block only runs when the script is executed directly, not when it's imported as a module.
- **`try:`**: This block tries to execute the main app logic.
    - **`# Main app execution`**: This is a placeholder comment where you would add any additional code that needs to run when the app starts. In this case, the main app logic is already handled by the code we've explained above, so this block is empty.
- **`except Exception as e:`**: This block catches any exceptions (errors) that occur during the execution of the `try` block.
    - **`handle_error(e)`**: If an exception occurs, it's passed to the `handle_error` function, which displays an error message to the user.

This structure ensures that the app starts up correctly and handles any unexpected errors gracefully.

## üìù Function Definitions: The Building Blocks of Our App

Now let's take a closer look at the functions we've defined:

### üì• `process_uploaded_file(uploaded_file, file_type)`: Reading File Content

```python
# Function to process uploaded file
def process_uploaded_file(uploaded_file, file_type):
    if file_type == "text/csv":
        df = pd.read_csv(uploaded_file)
        return df.to_string()
    elif file_type == "application/pdf":
        # PDF handling using PyPDF2
        pdf_reader = PyPDF2.PdfReader(uploaded_file)
        num_pages = len(pdf_reader.pages)
        content = ""
        for page_num in range(num_pages):
            page = pdf_reader.pages[page_num]
            content += page.extract_text()
        return content
    elif file_type.startswith('text/'):  # Handle other text files
        content = uploaded_file.getvalue().decode("utf-8")
        return content
    else:  # Handle binary files
        content = uploaded_file.read()  # Read as bytes
        return content
```

This function is responsible for reading the content of uploaded files:

- **`def process_uploaded_file(uploaded_file, file_type):`**: This defines the function, taking an `uploaded_file` object and its `file_type` as input.
- **`if file_type == "text/csv":`**: This checks if the uploaded file is a CSV file.
    - **`df = pd.read_csv(uploaded_file)`**: If it's a CSV, we use pandas (`pd`) to read it into a DataFrame (`df`).
    - **`return df.to_string()`**: We then convert the DataFrame to a string and return it.
- **`elif file_type == "application/pdf":`**: This checks if the uploaded file is a PDF file.
    - **`pdf_reader = PyPDF2.PdfReader(uploaded_file)`**: If it's a PDF, we use PyPDF2 to create a `PdfReader` object.
    - **`num_pages = len(pdf_reader.pages)`**: We get the number of pages in the PDF.
    - **`content = ""`**: We initialize an empty string to store the extracted text.
    - **`for page_num in range(num_pages):`**: This loop iterates through each page of the PDF.
        - **`page = pdf_reader.pages[page_num]`**: We get the current page.
        - **`content += page.extract_text()`**: We extract the text from the page and append it to the `content` string.
    - **`return content`**: We return the extracted text from all pages.
- **`elif file_type.startswith('text/'):`**: This checks if the uploaded file is a text file.
    - **`content = uploaded_file.getvalue().decode("utf-8")`**: We read the file content as bytes and decode it as UTF-8 text.
    - **`return content`**: We return the decoded text.
- **`else:`**: This block runs if the uploaded file is not a CSV, PDF, or text file.
    - **`content = uploaded_file.read()  # Read as bytes`**: We read the file content as bytes.
    - **`return content`**: We return the raw bytes of the file.

This function handles different file types, ensuring that we can extract the text content from CSV, PDF, and other text-based files, as well as handle binary files.

### üîó `get_download_link(content, filename, text)`: Creating Download Links

```python
# Function to generate a download link
def get_download_link(content, filename, text):
    b64 = base64.b64encode(content.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'
```

This function creates download links for text content:

- **`def get_download_link(content, filename, text):`**: This defines the function, taking the following arguments:
    - **`content`**: The text content to be downloaded.
    - **`filename`**: The desired filename for the downloaded file.
    - **`text`**: The text to be displayed on the download link.
- **`b64 = base64.b64encode(content.encode()).decode()`**: This encodes the text content in base64 format. Base64 encoding is a way to represent binary data (like text) as a string of ASCII characters, making it safe to include in URLs.
- **`return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'`**: This returns an HTML anchor tag (`<a>`) that creates a download link. The link's `href` attribute uses a data URL, which embeds the base64-encoded content directly in the link. The `download` attribute specifies the filename for the downloaded file.

This function allows users to easily download the generated prompts and test data as text files.

### üß† `get_system_prompt(task, variables, model, temperature, max_output_tokens)`: Crafting the System Prompt

```python
# New function to get system prompt
def get_system_prompt(task, variables, model, temperature, max_output_tokens):
    return f"""You are an advanced AI prompt engineer assistant integrated into a Streamlit app. Your task is to generate a highly effective Chain of Thought (COT) prompt based on the user's input. Follow these steps:

1. Analyze the task:
   - Identify the main objective of the user's task
   - Determine the complexity and scope of the task
   - Consider any specific domain knowledge required

2. Evaluate the variables:
   - Examine the provided variables (if any)
   - Determine how these variables should be incorporated into the prompt
   - Consider additional relevant variables that might enhance the prompt

3. Consider the Groq model capabilities:
   - Adapt the prompt to leverage the strengths of the selected Groq model ({model})
   - Take into account the current temperature setting ({temperature}) and max token limit ({max_output_tokens})

4. Incorporate COT elements:
   - Break down the task into logical steps or components
   - Include prompts for explanations or reasoning at each step
   - Encourage the model to show its work or thought process

5. Optimize for clarity and specificity:
   - Use clear, concise language
   - Avoid ambiguity in instructions
   - Include specific examples or constraints where appropriate

6. Add context and formatting instructions:
   - Provide any necessary background information
   - Specify desired output format (e.g., bullet points, paragraphs, JSON)
   - Include any relevant data or file analysis instructions if applicable

7. Encourage creativity and problem-solving:
   - Include prompts for alternative approaches or solutions
   - Ask for pros and cons of different methods if relevant

8. Incorporate error handling and edge cases:
   - Prompt the model to consider potential issues or limitations
   - Ask for validation steps or error checking where appropriate

9. Format the final prompt:
   - Present the COT prompt in a clear, structured manner
   - Use appropriate line breaks, numbering, or bullet points for readability

Generate a COT prompt that follows these steps and is optimized for the given task, variables, and selected Groq model. Ensure the prompt is detailed enough to guide the AI but concise enough to fit within token limits.

Task: {task}
Variables: {variables}
Selected Model: {model}
Temperature: {temperature}
Max Tokens: {max_output_tokens}

Based on the above information, generate an optimal Chain of Thought prompt:
"""
```

This function is the mastermind behind generating effective Chain of Thought (COT) prompts:

- **`def get_system_prompt(task, variables, model, temperature, max_output_tokens):`**: This defines the function, taking the user's task, variables, selected Groq model, temperature, and max output tokens as input.
- **`return f"""..."""`**: This returns a multi-line string that acts as a detailed instruction set for the Groq AI. It guides the AI through a step-by-step process to generate a high-quality COT prompt.

Let's break down the instructions within the system prompt:

1. **Analyze the task**: The AI is instructed to understand the user's goal, the complexity of the task, and any specific knowledge needed.
2. **Evaluate the variables**: The AI should consider how to incorporate the user-provided variables and think about any additional variables that might be helpful.
3. **Consider the Groq model capabilities**: The AI should tailor the prompt to the strengths of the chosen Groq model, taking into account the temperature and token limit settings.
4. **Incorporate COT elements**: The AI should break down the task into steps, encourage explanations, and prompt the model to show its thought process.
5. **Optimize for clarity and specificity**: The prompt should be clear, concise, and avoid ambiguity.
6. **Add context and formatting instructions**: The AI should include any necessary background information, specify the desired output format, and provide data analysis instructions if needed.
7. **Encourage creativity and problem-solving**: The prompt should encourage the AI to explore different approaches and solutions.
8. **Incorporate error handling and edge cases**: The AI should prompt the model to consider potential issues and include error checking steps.
9. **Format the final prompt**: The prompt should be well-structured and easy to read.

The system prompt then provides the AI with the specific task, variables, model version, temperature, and max tokens, and asks it to generate an optimal COT prompt based on all this information.

### ü§ñ `generate_prompt(task, variables="")`: Generating the Final Prompt

```python
@traceable # Langsmith Tracing and Observability
# Function to generate prompt
def generate_prompt(task, variables=""):
    client = Groq(api_key=groq_api_key)
    system_prompt = get_system_prompt(
        task, 
        variables, 
        st.session_state.model, 
        st.session_state.temperature, 
        st.session_state.max_output_tokens
    )
    
    try:
        chat_completion = client.chat.completions.create(  # Use chat.completions.create
            model=st.session_state.model,
            messages=[
                {"role": "system", "content": st.session_state.system_prompt},
                {"role": "user", "content": system_prompt}
            ],
            stream=True,
        )

        generated_prompt = ""
        for chunk in chat_completion:
            if chunk.choices[0].delta.content is not None:
                generated_prompt += chunk.choices[0].delta.content
                # You can update the UI with each chunk here if needed
                # For example: st.write(chunk.choices[0].delta.content, end='')

        return generated_prompt.strip()
    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        return None
```

This function takes the user's input and uses the Groq AI to generate the final prompt:

- **`@traceable # Langsmith Tracing and Observability`**: This decorator is used for tracking and monitoring the performance of the function using Langsmith.
- **`def generate_prompt(task, variables=""):`**: This defines the function, taking the user's task and variables as input.
- **`client = Groq(api_key=groq_api_key)`**: This initializes the Groq client using the API key.
- **`system_prompt = get_system_prompt(...)`**: This calls the `get_system_prompt` function to create the detailed instruction set for the AI, passing the user's task, variables, and the current model settings.
- **`try:`**: This block attempts to generate the prompt using the Groq AI.
    - **`chat_completion = client.chat.completions.create(...)`**: This uses the Groq API's `chat.completions.create` method to generate the prompt based on the `system_prompt`. The `messages` parameter includes the system prompt and the user's prompt. The `stream=True` parameter enables streaming responses.
    - **`generated_prompt = ""`**: We initialize an empty string to store the generated prompt.
    - **`for chunk in chat_completion:`**: This loop iterates through the chunks of the streaming response.
        - **`if chunk.choices[0].delta.content is not None:`**: This checks if the current chunk contains content.
            - **`generated_prompt += chunk.choices[0].delta.content`**: We append the content of the chunk to the `generated_prompt` string.
            - **`# You can update the UI with each chunk here if needed`**: This comment suggests that you can add code to update the UI with each chunk as it arrives, providing a real-time display of the generated prompt.
    - **`return generated_prompt.strip()`**: If the generation is successful, we return the generated prompt after removing any leading or trailing whitespace.
- **`except Exception as e:`**: This block catches any errors that occur during the generation process.
    - **`st.error(f"An error occurred: {str(e)}")`**: If an error occurs, we display an error message to the user.
    - **`return None`**: We return `None` to indicate that the generation failed.

This function orchestrates the entire prompt generation process, from crafting the system prompt to handling potential errors and enabling streaming responses.

### üé≤ `generate_test_data(topic, num_pairs)`: Creating Test Data

```python
@traceable # Langsmith Tracing and Observability
# Function to generate test data
def generate_test_data(topic, num_pairs):
    client = Groq(api_key=groq_api_key)
    prompt = f"""Generate {num_pairs} pairs of conversation for the topic: {topic}. 
    Each pair should consist of a human message and an AI response. 
    Format the output as a valid JSON array of objects, where each object has 'human' and 'ai' keys.
    Ensure the output is strictly in this format:
    [
        {{"human": "Human message 1", "ai": "AI response 1"}},
        {{"human": "Human message 2", "ai": "AI response 2"}},
        ...
    ]
    Do not include any text before or after the JSON array.
    """
    
    try:
        chat_completion = client.chat.completions.create(  # Use chat.completions.create
            model=st.session_state.model,
            messages=[
                {"role": "system", "content": st.session_state.system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
        )

        test_data_str = ""
        for chunk in chat_completion:
            if chunk.choices[0].delta.content is not None:
                test_data_str += chunk.choices[0].delta.content

        # Attempt to parse the response as JSON
        try:
            json_data = json.loads(test_data_str)
            if isinstance(json_data, list) and all(isinstance(item, dict) and 'human' in item and 'ai' in item for item in json_data):
                return json_data
            else:
                raise ValueError("Response is not in the expected format")
        except json.JSONDecodeError:
            # If JSON parsing fails, try to extract JSON from the response
            json_match = re.search(r'\[.*\]', test_data_str, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                return json.loads(json_str)
            else:
                raise ValueError("Could not extract valid JSON from the response")
    except Exception as e:
        st.error(f"An error occurred while generating test data: {str(e)}")
        return None
```

This function generates test data in the form of conversation pairs:

- **`@traceable # Langsmith Tracing and Observability`**: This decorator is used for tracking and monitoring the function's performance using Langsmith.
- **`def generate_test_data(topic, num_pairs):`**: This defines the function, taking the topic and the number of conversation pairs as input.
- **`client = Groq(api_key=groq_api_key)`**: This initializes the Groq client using the API key.
- **`prompt = f"""..."""`**: This creates a multi-line string that acts as a prompt for the Groq AI. The prompt instructs the AI to generate the specified number of conversation pairs on the given topic, formatted as a JSON array.
- **`try:`**: This block attempts to generate the test data using the Groq AI.
    - **`chat_completion = client.chat.completions.create(...)`**: This uses the Groq API's `chat.completions.create` method to generate the test data based on the `prompt`. The `messages` parameter includes the system prompt and the user's prompt. The `stream=True` parameter enables streaming responses.
    - **`test_data_str = ""`**: We initialize an empty string to store the generated test data as it streams in.
    - **`for chunk in chat_completion:`**: This loop iterates through the chunks of the streaming response.
        - **`if chunk.choices[0].delta.content is not None:`**: This checks if the current chunk contains content.
            - **`test_data_str += chunk.choices[0].delta.content`**: We append the content of the chunk to the `test_data_str` string.
    - **`try:`**: This inner `try` block attempts to parse the accumulated `test_data_str` as JSON.
        - **`json_data = json.loads(test_data_str)`**: This tries to convert the AI's response text into a JSON object.
        - **`if isinstance(json_data, list) and all(isinstance(item, dict) and 'human' in item and 'ai' in item for item in json_data):`**: This checks if the parsed JSON is a list of dictionaries, where each dictionary has the keys "human" and "ai", as specified in the prompt.
            - **`return json_data`**: If the format is correct, we return the parsed JSON data.
        - **`else:`**: This block runs if the parsed JSON doesn't match the expected format.
            - **`raise ValueError("Response is not in the expected format")`**: This raises an error indicating that the AI's response is not in the correct format.
    - **`except json.JSONDecodeError:`**: This block catches errors that occur if the AI's response is not valid JSON.
        - **`json_match = re.search(r'\[.*\]', test_data_str, re.DOTALL)`**: This tries to extract the JSON array from the response using regular expressions.
        - **`if json_match:`**: This checks if a JSON array was found.
            - **`json_str = json_match.group(0)`**: We extract the matched JSON string.
            - **`return json.loads(json_str)`**: We parse the extracted JSON string and return the result.
        - **`else:`**: This block runs if no JSON array was found in the response.
            - **`raise ValueError("Could not extract valid JSON from the response")`**: This raises an error indicating that valid JSON could not be extracted from the AI's response.
- **`except Exception as e:`**: This block catches any other errors that occur during the test data generation process.
    - **`st.error(f"An error occurred while generating test data: {str(e)}")`**: We display an error message to the user.
    - **`return None`**: We return `None` to indicate that the generation failed.

This function ensures that the generated test data is in the correct format and handles potential errors during the generation and parsing process. It also utilizes streaming responses from the Groq API.

## üèÅ Wrapping Up: The Complete Picture

We've now explored every line of code in our AI Prompt Generator app. You've seen how we import the necessary tools, set up the user interface, handle user input, generate AI prompts, create test data, provide help and documentation, and handle errors gracefully.

This app is a testament to the power of AI and its ability to enhance creativity, automate tasks, and provide valuable insights. By understanding the code, you can gain a deeper appreciation for how AI works and how you can leverage it to solve problems and explore new possibilities.

Remember, this is just the beginning. The world of AI is vast and constantly evolving. With the knowledge you've gained, you can continue to explore, experiment, and create amazing things with AI.




